{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb9f75624196753",
   "metadata": {},
   "source": [
    "# 0. 导库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 在文件开头添加导入\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba8dbb17e256f2",
   "metadata": {},
   "source": [
    "# 1. 创建数据集和数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0d4fa1712c747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_lora_ranks(model, train_dataset, data_collator, training_args, logger, calib_size, budget_rank, num_batches=8):\n",
    "    \"\"\"\n",
    "    使用稳定秩校准每层的LoRA秩\n",
    "\n",
    "    Args:\n",
    "        model: 待校准的模型\n",
    "        train_dataset: 训练数据集\n",
    "        data_collator: 数据收集器\n",
    "        training_args: 训练参数\n",
    "        logger: 日志器\n",
    "        calib_size: 校准批次大小\n",
    "        budget_rank: 总秩预算\n",
    "    \"\"\"\n",
    "    logger.info(\"*** Calibrating LoRA ranks using Stable Rank ***\")\n",
    "\n",
    "    # 设置模型为训练模式\n",
    "    model.train()\n",
    "    original_training = model.training\n",
    "\n",
    "    try:\n",
    "        # 创建校准数据加载器\n",
    "        # calib_dataset = train_dataset.select(range(min(calib_size * num_batches, len(train_dataset))))\n",
    "        # 直接使用完整数据集\n",
    "        calib_dataset = train_dataset\n",
    "\n",
    "        calib_dataloader = DataLoader(\n",
    "            calib_dataset,\n",
    "            batch_size=calib_size,\n",
    "            collate_fn=data_collator,\n",
    "            shuffle=True  # 打乱数据\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b8b7ce30292251",
   "metadata": {},
   "source": [
    "# 2. 进行批次遍历"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae79e0b5f313a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # 初始化统计容器和计时器\n",
    "        all_batch_metrics = []\n",
    "        batch_times = []\n",
    "\n",
    "        # 遍历多个批次\n",
    "        for i, batch in enumerate(tqdm(calib_dataloader, desc=\"Processing batches\", total=None)):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "\n",
    "            start_time = time.time()  # 记录批次开始时间\n",
    "\n",
    "            batch = next(iter(calib_dataloader))\n",
    "            batch = {k: v.to(training_args.device) if isinstance(v, torch.Tensor) else v\n",
    "                     for k, v in batch.items()}\n",
    "\n",
    "            # 前向 + 反向\n",
    "            model.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            # 收集当前批次的稳定秩和Frobenius范数\n",
    "            batch_layer_metrics = {}\n",
    "            for name, param in model.named_parameters():\n",
    "                # 条件1：必须是权重矩阵（排除bias/LayerNorm）\n",
    "                # 条件2：是2D矩阵\n",
    "                # 条件3：是query或value投影层（根据模型结构，只有这些层有LoRA）\n",
    "                if (param.grad is not None and\n",
    "                        'weight' in name and\n",
    "                        param.grad.dim() == 2 and\n",
    "                        ('attention.self.query.weight' in name or\n",
    "                         'attention.self.value.weight' in name)):\n",
    "\n",
    "                    G = param.grad.data\n",
    "\n",
    "                    # 计算Frobenius范数\n",
    "                    fro_norm = torch.norm(G, p='fro')\n",
    "\n",
    "                    # 使用SVD计算最大奇异值（谱范数）\n",
    "                    try:\n",
    "                        U, S, V = torch.svd_lowrank(G, q=1, niter=10)\n",
    "                        spec_norm = S[0]\n",
    "                    except:\n",
    "                        # 备用方法：直接计算矩阵范数\n",
    "                        spec_norm = torch.norm(G, p=2)\n",
    "\n",
    "                    # 计算稳定秩\n",
    "                    sr = (fro_norm ** 2) / (spec_norm ** 2 + 1e-12)\n",
    "\n",
    "                    # 构建复合分数：稳定秩 × Frobenius范数\n",
    "                    composite_score = sr.item() * fro_norm.item()\n",
    "\n",
    "                    batch_layer_metrics[name] = {\n",
    "                        'stable_rank': sr.item(),\n",
    "                        'fro_norm': fro_norm.item(),\n",
    "                        'composite_score': composite_score\n",
    "                    }\n",
    "\n",
    "            all_batch_metrics.append(batch_layer_metrics)\n",
    "            batch_times.append(time.time() - start_time)  # 记录批次耗时\n",
    "            logger.info(f\"Processed batch {i+1}/{num_batches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c35129368db4ea",
   "metadata": {},
   "source": [
    "# 3. 聚合多批次结果并打印"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b624e3481ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # 计算平均用时\n",
    "        avg_batch_time = np.mean(batch_times) if batch_times else 0\n",
    "\n",
    "        # 聚合多批次结果（取中位数减少异常值影响）\n",
    "        aggregated_metrics = {}\n",
    "        layer_names = list(all_batch_metrics[0].keys()) if all_batch_metrics else []\n",
    "\n",
    "        for name in layer_names:\n",
    "            stable_ranks = [bm[name]['stable_rank'] for bm in all_batch_metrics if name in bm]\n",
    "            fro_norms = [bm[name]['fro_norm'] for bm in all_batch_metrics if name in bm]\n",
    "            composite_scores = [bm[name]['composite_score'] for bm in all_batch_metrics if name in bm]\n",
    "\n",
    "            aggregated_metrics[name] = {\n",
    "                # 中位数更鲁棒（jk：要考虑所有数据点，均值对最后微调后的性能最有帮助）\n",
    "                'stable_rank': np.median(stable_ranks),\n",
    "                'fro_norm': np.median(fro_norms),\n",
    "                'composite_score': np.median(composite_scores)\n",
    "                # 均值\n",
    "                # 'stable_rank': np.mean(stable_ranks),  # 改为均值\n",
    "                # 'fro_norm': np.mean(fro_norms),  # 改为均值\n",
    "                # 'composite_score': np.mean(composite_scores)  # 改为均值\n",
    "            }\n",
    "\n",
    "        if not aggregated_metrics:\n",
    "            logger.warning(\"No gradient information found for LoRA calibration\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259637054d918051",
   "metadata": {},
   "source": [
    "# 4. 计算归一化分数并分配LoRA秩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0364e871da8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # 计算每层的归一化分数\n",
    "        composite_scores = np.array([metrics['composite_score'] for metrics in aggregated_metrics.values()])\n",
    "        softmax_scores = np.exp(composite_scores) / np.sum(np.exp(composite_scores))\n",
    "\n",
    "        # 分配LoRA秩\n",
    "        allocated_ranks = {}\n",
    "        for i, layer_name in enumerate(aggregated_metrics.keys()):\n",
    "            rank = int(round(softmax_scores[i] * budget_rank))  # 添加round()实现四舍五入\n",
    "            allocated_ranks[layer_name] = rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd86b7f71d78a8fa",
   "metadata": {},
   "source": [
    "# 5. 打印最终结果并恢复模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8eeb937e00a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # 打印校准结果\n",
    "        logger.info(\"=== LoRA Rank Calibration Results ===\")\n",
    "        for layer_name, metrics in aggregated_metrics.items():\n",
    "            rank = allocated_ranks.get(layer_name, 1)\n",
    "            logger.info(\n",
    "                f\"{layer_name}: \"\n",
    "                f\"SR={metrics['stable_rank']:.3f}, \"\n",
    "                f\"Fro={metrics['fro_norm']:.3f}, \"\n",
    "                f\"Score={metrics['composite_score']:.3f}, \"\n",
    "                f\"Rank={rank}\"\n",
    "            )\n",
    "\n",
    "        # 这里可以添加代码来动态修改模型的LoRA配置\n",
    "        # 例如：model.update_lora_ranks(allocated_ranks)\n",
    "        logger.info(f\"Total budget rank: {budget_rank}\")\n",
    "        logger.info(f\"Actual allocated: {sum(allocated_ranks.values())}\")\n",
    "        logger.info(f\"Average time per batch: {avg_batch_time:.2f} seconds\")  # 打印平均用时\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during LoRA calibration: {e}\")\n",
    "    finally:\n",
    "        # 恢复模型原始状态\n",
    "        model.train(original_training)\n",
    "        model.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
